{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_review_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu7Nynjse9f9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "b2a05c62-68a6-4f19-f053-b770c9ac8c0b"
      },
      "source": [
        "%tensorflow_version 2.x  \n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence #for padding\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE) # imdb dataset built-in in keras\n",
        "\n",
        "#Padding if data > 250 we'll trim workds to 250 and if data < 250 than we'll pad 0s at the left side\n",
        "\n",
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.4157 - acc: 0.8136 - val_loss: 0.2878 - val_acc: 0.8890\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.2371 - acc: 0.9100 - val_loss: 0.2763 - val_acc: 0.8930\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.1834 - acc: 0.9323 - val_loss: 0.3360 - val_acc: 0.8694\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.1510 - acc: 0.9457 - val_loss: 0.2868 - val_acc: 0.8974\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.1289 - acc: 0.9551 - val_loss: 0.3394 - val_acc: 0.8880\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.1141 - acc: 0.9614 - val_loss: 0.2842 - val_acc: 0.8856\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.0989 - acc: 0.9669 - val_loss: 0.3811 - val_acc: 0.8442\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.0884 - acc: 0.9709 - val_loss: 0.3132 - val_acc: 0.8860\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0761 - acc: 0.9742 - val_loss: 0.3486 - val_acc: 0.8842\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.0686 - acc: 0.9784 - val_loss: 0.3986 - val_acc: 0.8880\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.5225 - acc: 0.8520\n",
            "[0.522453248500824, 0.8519999980926514]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV0yyReYihlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ba95bb08-e1a1-4dbc-d70b-6a3c8743a5cf"
      },
      "source": [
        "#encoding the review\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbrW84F4iu9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "72f7472a-0c89-415e-f8aa-c09a2ded186c"
      },
      "source": [
        "# extra step, not needed. decoding the review\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(test_data[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the wonder own as by is sequence i i jars roses to of hollywood br of down shouting getting boring of ever it sadly sadly sadly i i was then does don't close faint after one carry as by are be favourites all family turn in does as three part in another some to be probably with world uncaring her an have faint beginning own as is sequence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOZ8gsc_jCET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6987618-d9b9-4c80-8ac0-97ceca9e9a5b"
      },
      "source": [
        "# PREDICTIONS\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was amazing! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.92351294]\n",
            "[0.3178439]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}